{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5b987c-a769-4026-8625-9cdb7872c066",
   "metadata": {},
   "source": [
    "# Task 1: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fda2ad8-9914-46b9-8d24-4c4cc678b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prepare a model for glass classification using Random Forest\n",
      "0                                  Data Description:          \n",
      "1                              RI : refractive index          \n",
      "2  Na: Sodium (unit measurement: weight percent i...          \n",
      "3                                      Mg: Magnesium          \n",
      "4                                       AI: Aluminum          \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 1 columns):\n",
      " #   Column                                                        Non-Null Count  Dtype \n",
      "---  ------                                                        --------------  ----- \n",
      " 0   Prepare a model for glass classification using Random Forest  18 non-null     object\n",
      "dtypes: object(1)\n",
      "memory usage: 284.0+ bytes\n",
      "None\n",
      "Prepare a model for glass classification using Random Forest    1\n",
      "dtype: int64\n",
      "       Prepare a model for glass classification using Random Forest\n",
      "count                                                  18          \n",
      "unique                                                 18          \n",
      "top                                     Data Description:          \n",
      "freq                                                    1          \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(r\"C:\\Users\\sai\\OneDrive\\Documents\\glass.xlsx\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Check the structure of the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Summary statistics of the dataset\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c8b50-3baa-4599-9679-aa18c89d7f66",
   "metadata": {},
   "source": [
    "# Task 2: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e31627-4eb3-4256-aac4-ac856050cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare a model for glass classification using Random Forest    object\n",
      "dtype: object\n",
      "DataFrame is empty: False\n",
      "  Prepare a model for glass classification using Random Forest\n",
      "0                                  Data Description:          \n",
      "1                              RI : refractive index          \n",
      "2  Na: Sodium (unit measurement: weight percent i...          \n",
      "3                                      Mg: Magnesium          \n",
      "4                                       AI: Aluminum          \n",
      "No numerical columns found for plotting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(r\"C:\\Users\\sai\\OneDrive\\Documents\\glass.xlsx\")\n",
    "\n",
    "# Print data types of each column\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check if the dataframe is empty\n",
    "print(\"DataFrame is empty:\", df.empty)\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "# Select numerical columns again, just in case\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "if not numerical_cols.empty:\n",
    "    # Plot histograms for numerical columns\n",
    "    df[numerical_cols].hist(bins=15, figsize=(15, 10))\n",
    "    plt.show()\n",
    "\n",
    "    # Box plot to check for outliers\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.boxplot(data=df[numerical_cols])\n",
    "    plt.show()\n",
    "\n",
    "    # Pair plot to visualize relationships\n",
    "    sns.pairplot(df, hue='Type')  # Assuming 'Type' is categorical and exists\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation heatmap for numerical columns\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df[numerical_cols].corr(), annot=True, cmap='coolwarm')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical columns found for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e421a1-8be7-407f-ae1f-1b053784af27",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3478350-fd8b-4a77-aeba-05aa60ac2e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No numerical columns found for imputation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel(r\"C:\\Users\\sai\\OneDrive\\Documents\\glass.xlsx\")\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(\"The dataset is empty!\")\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Check if the numerical columns are valid (not empty)\n",
    "if len(numerical_columns) > 0:\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    df[numerical_columns] = imputer_num.fit_transform(df[numerical_columns])\n",
    "else:\n",
    "    print(\"No numerical columns found for imputation.\")\n",
    "\n",
    "# Check if the categorical columns are valid (not empty)\n",
    "if len(categorical_columns) > 0:\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    df[categorical_columns] = imputer_cat.fit_transform(df[categorical_columns])\n",
    "else:\n",
    "    print(\"No categorical columns found for imputation.\")\n",
    "\n",
    "# One-hot encoding\n",
    "if len(categorical_columns) > 0:\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "else:\n",
    "    df_encoded = df\n",
    "\n",
    "# Feature scaling\n",
    "if not df_encoded.empty:\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_encoded), columns=df_encoded.columns)\n",
    "else:\n",
    "    raise ValueError(\"Encoded DataFrame is empty!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f226302b-1c88-4432-9ecc-684862a94810",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'Data Description:'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Handle missing values (if any)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m df_imputed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(df), columns\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Encode categorical variables (if any) - Assuming 'Type' is the target column and no categorical features\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# If you have categorical features, you would use pd.get_dummies or similar here\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Feature scaling\u001b[39;00m\n\u001b[0;32m     20\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:390\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    382\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter was deprecated in version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.1 and will be removed in 1.3. A warning will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    388\u001b[0m     )\n\u001b[1;32m--> 390\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(X, in_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:342\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n\u001b[0;32m    337\u001b[0m     new_ve \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m strategy with non-numeric data:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    339\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, ve\n\u001b[0;32m    340\u001b[0m         )\n\u001b[0;32m    341\u001b[0m     )\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ve\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'Data Description:'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Copy the file to a simpler location like C:\\ and then try accessing it\n",
    "df = pd.read_excel(r\"C:\\Users\\sai\\OneDrive\\Documents\\glass.xlsx\")\n",
    "\n",
    "# Handle missing values (if any)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Encode categorical variables (if any) - Assuming 'Type' is the target column and no categorical features\n",
    "# If you have categorical features, you would use pd.get_dummies or similar here\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(df_imputed.drop('Type', axis=1)), columns=df_imputed.drop('Type', axis=1).columns)\n",
    "\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X = X_scaled\n",
    "y = df_imputed['Type']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier instance\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69cf5f-efe7-4b00-a116-4991a9fb334a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9b2a7-40ce-4ced-b141-e0ffb1ae2732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
